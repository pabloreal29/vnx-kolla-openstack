Importante, utilizar la rama antelope.

SITUACIÓN ACTUAL:

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
LOAD BALANCER
	- Octavia parece que se arranca en el nodo Network, viendo los logs del despliegue de Kolla.
	

	En la task Deploy si se hacen todos los cambios en globals.yml:

	PLAY [Apply role octavia]
	TASK [service-ks-register : octavia | Creating services] ***********************************************************************************
	FAILED - RETRYING: [10.0.0.11]: octavia | Creating services (5 retries left).
	FAILED - RETRYING: [10.0.0.11]: octavia | Creating services (4 retries left).
	FAILED - RETRYING: [10.0.0.11]: octavia | Creating services (3 retries left).
	FAILED - RETRYING: [10.0.0.11]: octavia | Creating services (2 retries left).
	FAILED - RETRYING: [10.0.0.11]: octavia | Creating services (1 retries left).
	
	failed: [10.0.0.11] (item=octavia (load-balancer)) => {"action": "os_keystone_service", "ansible_loop_var": "item", "attempts": 5, "changed": false, "item": {"description": "Octavia Load Balancing Service", "endpoints": [{"interface": "internal", "url": "http://10.0.0.250:9876"}, {"interface": "public", "url": "http://10.0.0.250:9876"}], "name": "octavia", "type": "load-balancer"}, "module_stderr": "Failed to discover available identity versions when contacting http://10.0.0.250:5000. Attempting to parse version from URL. Could not find versioned identity endpoints when attempting to authenticate. Please check that your auth_url is correct. Service Unavailable (HTTP 503)\n", "module_stdout": "", "msg": "MODULE FAILURE\nSee stdout/stderr for the exact error", "rc": 1}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

IMPORTANTE. Hay que quitar el openstack_core para que no de el error anterior:
	- Hay que asignar una IP a eth3 en controller (y no sé si en network también, creo que es en controller), con la IP de gateway_ip: "10.1.2.1". Par ello, acceder al nodo y realizar estos pasos:
		ip link add link eth3 name eth3.1005 type vlan id 1005
		ip link set dev eth3.1005 up
		ip addr add 10.1.2.1/24 dev eth3.1005
		
		ip link delete eth3.1005 #Para borrar la interfaz de red
	
	- Estoy probando a crear un balanceador de carga con Octavia.
	- Tengo que utilizar VlaNet como red de VLAN para gestionar el tráfico de mngmnt de Octavia. Sin embargo, esta red solo conecta el nodo de red y los de computación.
	- Por eso, hay que modificar el xml para añadir una interfaz de red para VlanNet en admin y controller. Ahora voy a probar a añadirla SOLO en controller, no en admin. Mismo error, el de arriba.
		- Si no lo hago, sale un error al desplegar OpenStack, al configurar la red de mngment de Octavia, que dice que no se encuentra la VlanNet.
	
	- Cambios en globals.yml:
			octavia_network_interface: "eth3" 
			
			octavia_auto_configure: yes
			octavia_amp_flavor:
			  flavorid: 255
			  name: "amphora"
			  is_public: no
			  vcpus: 1
			  ram: 512
			  disk: 5
			  
			# Octavia security groups. lb-mgmt-sec-grp is for amphorae.
			octavia_amp_security_groups:
			    mgmt-sec-grp:
			      name: lb-mgmt-sec-grp
			      rules:
				- protocol: icmp
				- protocol: tcp
				  src_port: 22
				  dst_port: 22
				- protocol: tcp
				  src_port: 5555
				  dst_port: 5555
			 
			octavia_amp_network:
			  name: lb-mgmt-net
			  provider_network_type: vlan
			  provider_segmentation_id: 1005
			  provider_physical_network: physnet1
			  external: false
			  shared: false
			  subnet:
			    name: lb-mgmt-subnet
			    cidr: "10.1.2.0/24"
			    allocation_pool_start: "10.1.2.100"
			    allocation_pool_end: "10.1.2.200"
			    gateway_ip: "10.1.2.1"
			    enable_dhcp: yes


			octavia_amp_image_tag: "amphora"

			# Load balancer topology options are [ SINGLE, ACTIVE_STANDBY ]
			octavia_loadbalancer_topology: "SINGLE"
			
			# The following variables are ignored as along as `octavia_auto_configure` is set to `yes`.
			#octavia_amp_image_owner_id:
			octavia_amp_boot_network_list: lb-mngmt-net
			octavia_amp_secgroup_list: lb-mgmt-sec-grp
			octavia_amp_flavor_id: 255
		
		
	- El error que sale es que no detecta la imagen de amphora (recuerda que la he importado yo de VNX, no he seguido la receta de Kolla).
	- ERROR YA SUPERADO!! Este error se puede ver en el nodo controller, dentro de /var/log/kolla/octavia/octavia-worker.log.
		2024-04-26 09:38:29.193 17 ERROR octavia.compute.drivers.nova_driver octavia.common.exceptions.ImageGetException: Failed to retrieve image with amphora tag.
		2024-04-26 09:38:29.193 17 ERROR octavia.compute.drivers.nova_driver 
		2024-04-26 09:38:29.193 17 ERROR octavia.controller.worker.v2.tasks.compute_tasks [-] Compute create for amphora id: 7eb70294-caaa-4e0f-b3d8-e4ef524472a2 failed:
		octavia.common.exceptions.ComputeBuildException: Failed to build compute instance due to: Failed to retrieve image with amphora tag.
		
	- Cambiando el id del owner de octavia en el nodo controlador consigo quitarme ese error, ahora me dice: No valid host was found:
	- El error es:
		2024-04-26 11:29:17.332 18 ERROR octavia.common.base_taskflow octavia.common.exceptions.ComputeBuildException: Failed to build compute instance due to: {'code': 500, 'created': '2024-04-26T11:29:14Z', 'message': 'No valid host was found. ', 'details': 'Traceback (most recent call last):\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/nova/conductor/manager.py", line 1654, in schedule_and_build_instances\n    host_lists = self._schedule_instances(context, request_specs[0],\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/nova/conductor/manager.py", line 942, in _schedule_instances\n    host_lists = self.query_client.select_destinations(\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/nova/scheduler/client/query.py", line 41, in select_destinations\n    return self.scheduler_rpcapi.select_destinations(context, spec_obj,\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/nova/scheduler/rpcapi.py", line 160, in select_destinations\n    return cctxt.call(ctxt, \'select_destinations\', **msg_args)\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/oslo_messaging/rpc/client.py", line 190, in call\n    result = self.transport._send(\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/oslo_messaging/transport.py", line 123, in _send\n    return self._driver.send(target, ctxt, message,\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 689, in send\n    return self._send(target, ctxt, message, wait_for_reply, timeout,\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 681, in _send\n    raise result\nnova.exception_Remote.NoValidHost_Remote: No valid host was found. \nTraceback (most recent call last):\n\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/oslo_messaging/rpc/server.py", line 244, in inner\n    return func(*args, **kwargs)\n\n  File "/var/lib/kolla/venv/lib/python3.10/site-packages/nova/scheduler/manager.py", line 243, in select_destinations\n    raise exception.NoValidHost(reason="")\n\nnova.exception.NoValidHost: No valid host was found. \n\n'}


	-Cambios a hacer:
		nano /etc/kolla/octavia-(todos)/octavia.conf y asignar en la propiedad amp_image_owner_id el id del project admin.
		systemctl restart kolla-octavia_api-container.service kolla-octavia_health_manager-container.service kolla-octavia_housekeeping-container.service kolla-octavia_worker-container.service
	- Quiero probar con la red de ayer y el nuevo cambio

	- Se puede cambiar el amp_image_owner_id directamente en globals.yml, poniendo octavia_auto_configure: no. Entonces, necesitaremos el octavia_managment_port y este es el 5555 por defecto.
	
	admin_project_id=$(openstack project show admin -c id -f value)
	sed -i "s/#octavia_amp_image_owner_id.*/octavia_amp_image_owner_id: $admin_project_id/" /etc/kolla/globals.yml

-------------------------------------------------------------------------------------------------------------------------------------------------------------
FIREWALL

	- No puedo crear el firewall: The resource could not be found. Neutron server returns request_ids: ['req-fc00a806-06ad-4b12-b668-f3f2be31f886']

------------------------------------------------------------------------------------------------------------------------------------------------------

PROBLEMAS ya SOLUCIONADOS

1. Problema de almacenamiento solucionado: ahora los nodos tienen 40G en vez de 20G.
Errores obtenidos:	
	Error 1: httpException: 413: Client Error for url:  Request Entity Too Large
	Error 2: HTTP 413 Request Entity Too Large: Image storage media is full: There is not enough disk space on the image storage media

2. Sin acceso SSH a admin. Solucionado:
	- David añade 2 líneas al .xml donde se cambia el MTU a 1400 (así se permite el acceso por SSH).
	- Si no, se puede cambiar a mano mediante: ip link set dev ens3 mtu 1400

3. Sin conectividad en net2: las interfaces de red se creaban en estado DOWN. 
	- Solución: Cambio en la configuracion de net2 eliminando el atributo gateway_ip.
	- Si no: 
		- Ejecutar: ip link set dev ens4 up. 
		- Luego eliminar la ruta que se ha creado: ip route del default via 10.1.2.1

-------------------------------------------------------------------------------------------------------------------------------------------------
PRUEBAS CON GLANCE

MétodoHTTP Backend NO FUNCIONA. HAY QUE PROBAR OTRO TIPO DE BACKENDS (SWIFT DIO ERROR EN LA CONF DE OPENSTACK)

Causas:
- No se puede realizar: openstack image create con la opción --location porque está deprecado en la v2 de Glance
- Otra opción es descargar la imagen con: wget http://10.0.0.1:80/servers_image.qcow2 -O servers_image.qcow2, pero luego da error: 
openstack image create --container-format bare --disk-format qcow2 --public --file servers_image.qcow2 focal-servers-vnx
HttpException: 410: Client Error for url: http://10.0.0.250:9292/v2/images/1aa8dfc8-4e96-47bb-ba40-84948186beab/file, Gone
- El error radica en que no se permite en algún sitio añadir imágenes al backend pero no sé dónde (creo que el backend de tipo http en Glance v2 va mal y ya):
2024-04-17 18:39:30.012 18 ERROR glance.api.v2.image_data glance_store.exceptions.StoreAddDisabled: Configuration for store failed. Adding images to this store is disabled. 


1. Acceder al nodo Controller, actualizar esto para obtener las imagenes con glance mediante http:
[glance_store]
stores = http
default_store = http
http_store_host = 10.0.0.1
http_store_port = 8000
default_backend = http

Ejecutar: systemctl restart kolla-glance_api-container.service

2. Subir las imagenes al nodo admin: scp -r ../openstack-images/ root@admin:/root/openstack-images

3. Acceder al nodo admin, a la ruta /root/openstack-images y ejecutar: python3 -m http.server 
3.1 Comprobar si el servidor está bien lanzado ejecutando desde otro nodo (no admin): curl -I http://10.0.0.1:8000/servers_image.qcow2

MÁS ALTERNATIVAS: Swift y Ceph no se habilitan correctamente en la configuración. Cinder necesita de una imagen de Glance, no sirve.

-------------------------------------------------------------------------------------------------------------------------------------------------
DESCRIPCIÓN ESCENARIO

El escenario es el siguiente:
compute3: [10.0.0.32]
compute2: [10.0.0.33]
compute1: [10.0.0.31]
network: [10.0.0.21]
controller: [10.0.0.11]
admin: [10.0.0.1]


pabloreal@giros21:~/vnx-kolla-openstack$ openstack --os-cloud kolla-admin service list
+----------------------------------+-----------+----------------+
| ID                               | Name      | Type           |
+----------------------------------+-----------+----------------+
| 1724d842ac544f778759b4e8903b4921 | heat-cfn  | cloudformation |
| 545af203e0bb4f46ae5d7700ae551eba | keystone  | identity       |
| 905bf5d1f9f04b9ca7cc4aabb0df2592 | neutron   | network        |
| beb1870570f04811820f45453152c95b | heat      | orchestration  |
| c640489bb1e446c6af76ff0648afa828 | gnocchi   | metric         |
| ebcd99cdc54d40b9a82489e605ef2031 | glance    | image          |
| ee34a153364245039e053ddd08518c89 | placement | placement      |
| f0470b6fd0f84475aae4e84c33dc2e09 | nova      | compute        |
+----------------------------------+-----------+----------------+


kolla-nova_api-container.service                                                                      loaded active     running   docker >
  kolla-nova_conductor-container.service                                                                loaded active     running   docker >
  kolla-nova_novncproxy-container.service                                                               loaded active     running   docker >
  kolla-nova_scheduler-container.service  


