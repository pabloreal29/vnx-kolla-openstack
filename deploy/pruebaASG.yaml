heat_template_version: 2018-08-31
description: Auto Scale Group, Policy and Alarm
parameters:
  image:
    type: string
    description: Image used to create instance
    default: focal-server-cloudimg-amd64-vnx
  flavor:
    type: string
    description: Instance flavor to be used
    default: m1.smaller
  key_name:
    type: string
    description: Keypair to be used
    default: s1
  network:
    type: string
    description: project network to attach instance to
    default: Net1
  security_groups:
    type: comma_delimited_list
    description: List of security group names or IDs
    default: open
  tags:
    type: comma_delimited_list
    description: Tags from the server
    default: []
  user_data:
    type: string
    description: Instance user_data
    default: |
      #!/bin/bash
      # DEBIAN_FRONTEND=noninteractive apt update
      # DEBIAN_FRONTEND=noninteractive apt install -y stress
      # nohup stress -c 4 &
  granularity:
    type: number
    description: The time range in seconds over which to query alarm
    default: 60
  cooldown:
    type: number
    description: Cooldown period, in seconds
    default: 60
  evaluation_periods:
    type: number
    description: Number of periods to evaluate over
    default: 1
  desired_capacity:
    type: number
    description: Desired initial number of resources
    default: 1
  min_size:
    type: number
    description: Minimum number of resources in the group
    default: 1
  max_size:
    type: number
    description: Maximum number of resources in the group
    default: 3
  # https://storyboard.openstack.org/#!/story/2007350
  # The formula is: time_ns = 1,000,000,000 x {granularity} x {percentage_in_decimal}.
  # Example of granularity of 300 values:
  # Granularity should match ceilometer  
  # CPU usage in %	Threshold
  # 100	300000000000.0
  # 90	  270000000000.0
  # 80   240000000000.0
  # 70	  210000000000.0
  # 60	  180000000000.0
  # 50	  150000000000.0
  # 40	  120000000000.0
  # 30	   90000000000.0
  # 20	   60000000000.0
  # 10	   30000000000.0

  cpu_threshold_high:
    type: number
    description: CPU Threshold to evaluate against to scale up
    default: 240000000000.0
  cpu_threshold_low:
    type: number
    description: CPU Threshold to evaluate against to scale down
    default: 90000000000.0

resources:
  asg:
    type: OS::Heat::AutoScalingGroup
    properties:
      cooldown: {get_param: cooldown}
      desired_capacity: {get_param: desired_capacity}
      max_size: {get_param: max_size}
      min_size: {get_param: min_size}
      resource:
        type: OS::Nova::Server
        properties:
          flavor: {get_param: flavor}
          image: {get_param: image}
          key_name: {get_param: key_name}
          security_groups: {get_param: security_groups}
          tags: {get_param: tags}
          networks: [{network: {get_param: network} }]
          user_data_format: RAW
          user_data: {get_param: user_data}
          metadata: {"metering.server_group": {get_param: "OS::stack_id"}}

  instance_scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: asg}
      cooldown: {get_param: cooldown}
      scaling_adjustment: 1

  instance_scaledown_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: asg}
      cooldown: {get_param: cooldown}
      scaling_adjustment: -1

  cpu_alarm_high:
    type: OS::Aodh::GnocchiAggregationByResourcesAlarm
    properties:
      description: Scale up if CPU > XX
      metric: cpu
      aggregation_method: rate:mean
      granularity: {get_param: granularity}
      evaluation_periods: {get_param: evaluation_periods}
      threshold: {get_param: cpu_threshold_high}
      resource_type: instance
      comparison_operator: gt
      alarm_actions:
        - str_replace:
            template: trust+url
            params:
              url: {get_attr: [instance_scaleup_policy, signal_url]}
      query:
        str_replace:
          template: '{"=": {"server_group": "stack_id"}}'
          params:
            stack_id: {get_param: "OS::stack_id"}

  cpu_alarm_low:
    type: OS::Aodh::GnocchiAggregationByResourcesAlarm
    properties:
      description: Scale down if CPU < XX
      metric: cpu
      aggregation_method: rate:mean
      granularity: {get_param: granularity}
      evaluation_periods: {get_param: evaluation_periods}
      threshold: {get_param: cpu_threshold_low}
      resource_type: instance
      comparison_operator: lt
      alarm_actions:
        - str_replace:
            template: trust+url
            params:
              url: {get_attr: [instance_scaledown_policy, signal_url]}
      query:
        str_replace:
          template: '{"=": {"server_group": "stack_id"}}'
          params:
            stack_id: {get_param: "OS::stack_id"}

outputs:
  scale_up_url:
    description: >
      This URL is the webhook to scale up the autoscaling group.  You
      can invoke the scale-up operation by doing an HTTP POST to this
      URL; no body nor extra headers are needed.
    value: {get_attr: [instance_scaleup_policy, alarm_url]}

  scale_down_url:
    description: >
      This URL is the webhook to scale down the autoscaling group.
      You can invoke the scale-down operation by doing an HTTP POST to
      this URL; no body nor extra headers are needed.
    value: {get_attr: [instance_scaledown_policy, alarm_url]}

  asg_size:
    description: This is the current size of the auto scaling group.
    value: {get_attr: [asg, current_size]}

  server_list:
    description: This is a list of server names that are part of the group.
    value: {get_attr: [asg, outputs_list, name]}

  server_ips:
    description: This is a list of first ip addresses of the servers in the group for a specified network.
    value: {get_attr: [asg, outputs_list, networks, {get_param: network}, 0]}

  networks:
    description: This is a map of server resources and their networks.
    value: {get_attr: [asg, outputs, networks]}